{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Functions for scraping Boligsiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get URL for Boligsiden search for specified period in selected Kommune\n",
    "\n",
    "def get_url_boligsiden(kommune, startdate, enddate, p):\n",
    "    url = 'http://www.boligsiden.dk/salgspris/solgt/alle/{}'\n",
    "    params = '?periode.from={}&periode.to={}&displaytab=mergedtab&sort' \\\n",
    "             '=salgsdato&salgstype=%5Bobject%20Object%5D&kommune={}'\n",
    "    full_url = url + params\n",
    "    return full_url.format(p, startdate, enddate, kommune)\n",
    "\n",
    "#### Get number of pages for Boligsiden search\n",
    "\n",
    "def get_max_pages_boligsiden(url):\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    driver = webdriver.Chrome('/Users/frederikmadsen/PycharmProjects/Thesis/chromedriver', options=options)\n",
    "    driver.get(url)\n",
    "    page_text = driver.find_element_by_class_name(\"salesprice-result\").text\n",
    "\n",
    "    last_page_num = (page_text.split(\"af \")[1]).split(\"\\n\")[0]\n",
    "    return last_page_num\n",
    "\n",
    "#### Get all address links on search page\n",
    "\n",
    "def get_all_urls_on_page_boligsiden(url):\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    driver = webdriver.Chrome('/Users/frederikmadsen/PycharmProjects/Thesis/chromedriver', options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    all_https = []\n",
    "    with_reentries_https = []\n",
    "\n",
    "    for elem in driver.find_elements_by_tag_name('a'):\n",
    "        all_https.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "    #bolig-links wanted appear multiple times, so we take away all single time occuring links\n",
    "    for i in range(len(all_https)):\n",
    "        if all_https[i] in all_https[:i]:\n",
    "            with_reentries_https.append(all_https[i])\n",
    "\n",
    "    #Take away first two entries, which are not bolig links\n",
    "    with_reentries_https = with_reentries_https[2:]\n",
    "\n",
    "    reduced_list = list(set(with_reentries_https))\n",
    "\n",
    "    #To make sure no other links are included\n",
    "    boliger_https = []\n",
    "    condition = 'https://www.boligsiden.dk/adresse/'\n",
    "    for i in reduced_list:\n",
    "        if condition in i:\n",
    "            boliger_https.append(i)\n",
    "\n",
    "    return boliger_https\n",
    "\n",
    "#### Get list of all address URLs for search\n",
    "\n",
    "def get_all_links_boligsiden(kommune, startdate, enddate):\n",
    "    # Returns first https-page with given variables\n",
    "    first_page = get_url_boligsiden(kommune, startdate, enddate, 1)\n",
    "\n",
    "    # Getting number of total pages\n",
    "    total_pages = get_max_pages_boligsiden(first_page)\n",
    "\n",
    "    # Empty lists\n",
    "    link_to_all_pages = []\n",
    "    list_of_all_pages = []\n",
    "\n",
    "    # Collects a list with all the pages that we want to collect\n",
    "    for x in tqdm(range(int(total_pages))):\n",
    "        all_pages = get_url_boligsiden(kommune, startdate, enddate, x + 1)\n",
    "        link_to_all_pages.append(all_pages)\n",
    "\n",
    "        page_list = get_all_urls_on_page_boligsiden(link_to_all_pages[x])\n",
    "        list_of_all_pages.extend(page_list)\n",
    "\n",
    "    # Returns list with all the wanted url's\n",
    "    return (list_of_all_pages)\n",
    "\n",
    "#### Scrape information for single address on address URL \n",
    "\n",
    "def get_simple_single_page_boligsiden(url):\n",
    "\n",
    "    url = url\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "    head = str(soup.find('head'))\n",
    "    try:\n",
    "        json_string = re.search(r'__bs_addresspresentation__ = ([^;]*)', head).group(1)\n",
    "        data = json.loads(json_string)\n",
    "        df1 = pd.json_normalize(data)\n",
    "        df2 = pd.DataFrame()\n",
    "    except:\n",
    "        json_string = re.search(r'__bs_propertypresentation__ = ([^;]*)', head).group(1)\n",
    "        data = json.loads(json_string)\n",
    "        df2 = pd.json_normalize(data)\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "#### Collect scraped information for all addresses in two dataframes\n",
    "\n",
    "def get_data_boligsiden(links):\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "\n",
    "    for x in tqdm(range(0, len(links))):\n",
    "        try:\n",
    "            df_pages1, df_pages2 = get_simple_single_page_boligsiden(links[x])\n",
    "            df1 = pd.concat([df1, df_pages1])\n",
    "            df2 = pd.concat([df2, df_pages2])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Functions for scraping DinGeo.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get DinGeo-URLs for all addresses in Boligsiden dataframes \n",
    "\n",
    "def get_geolinks1(df):\n",
    "    df[\"dingeo_link\"] = \"\"\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if '-' in (df['address.street'][x]):\n",
    "            df['address.street'][x] = df['address.street'].str.split('-').str[0][x] + '--' \\\n",
    "                                      + df['address.street'].str.split('-').str[1][x]\n",
    "\n",
    "        if ',' in (df['address.street'][x]):\n",
    "            add_part = str(df['address.postalId'][x]) + '-' + df['address.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(',').str[0][x].replace(\" \",\"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(', ').str[1][x].replace(\".\", \"\").replace(\" \", \"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + add_part\n",
    "        elif 'Adressen er ikke tilgængelig' in (df['address.street'][x]):\n",
    "            url = 'Utilgængelig'\n",
    "        else:\n",
    "            add_part = str(df['address.postalId'][x]) + '-' + df['address.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(',').str[0][x].replace(\" \",\"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + add_part\n",
    "\n",
    "        if '-lejl-' in url:\n",
    "            url = url.replace('-lejl-','-')\n",
    "\n",
    "        df['dingeo_link'][x] = url\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_geolinks2(df):\n",
    "    df[\"dingeo_link\"] = \"\"\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if '-' in (df['property.address'][x]):\n",
    "            df['property.address'][x] = df['property.address'].str.split('-').str[0][x] + '--' \\\n",
    "                                        + df['property.address'].str.split('-').str[1][x]\n",
    "\n",
    "        if ',' in (df['property.address'][x]):\n",
    "            ad_part = str(df['property.postal'][x]) + '-' + df['property.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(',').str[0][x].replace(\" \",\"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(', ').str[1][x].replace(\".\", \"\").replace(\" \", \"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + ad_part\n",
    "        elif 'Adressen er ikke tilgængelig' in (df['property.address'][x]):\n",
    "            url = 'Utilgængelig'\n",
    "        else:\n",
    "            ad_part = str(df['property.postal'][x]) + '-' + df['property.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(',').str[0][x].replace(\" \",\"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + ad_part\n",
    "\n",
    "        if '-lejl-' in url:\n",
    "            url = url.replace('-lejl-','-')\n",
    "\n",
    "        df['dingeo_link'][x] = url\n",
    "\n",
    "    return df\n",
    "\n",
    "#### Scrape information for each individual address on DinGeo.dk\n",
    "\n",
    "def dingeo_page(url):\n",
    "    url = url\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # Dictionary\n",
    "    data = {}\n",
    "    data['dingeo_link'] = url\n",
    "    try:\n",
    "        data['Radonrisiko'] = [soup.find_all(\"div\", {\"id\": 'radon'})[0].find_all(\"strong\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if 'ikke registreret trafikstøj' in soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].get_text():\n",
    "        data['Støjmåling'] = ['Ingen trafikstøj']\n",
    "    elif 'mangler desværre at indsamle trafikstøj' in soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].get_text():\n",
    "        data['Støjmåling'] = ['Mangler']\n",
    "    else:\n",
    "        data['Støjmåling'] = [soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].find_all(\"b\")[1].get_text()]\n",
    "\n",
    "    data['Oversvømmelsesrisiko_skybrud'] = [soup.find_all(\"div\", {\"id\": 'skybrud'})[0].find_all(\"b\")[0].get_text()]\n",
    "    data['Meter_over_havet'] = [soup.find_all(\"div\", {\"id\": 'stormflod'})[0].find_all(\"b\")[0].get_text()]\n",
    "\n",
    "    table_0 = pd.read_html(str(soup.find_all('table')))[0].iloc[:, 0:2]\n",
    "    table_0 = table_0.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_1 = pd.read_html(str(soup.find_all('table')))[1].iloc[:, 0:2]\n",
    "    table_1 = table_1.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_2 = pd.read_html(str(soup.find_all('table')))[2].iloc[:, 0:2]\n",
    "    table_2 = table_2.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_3 = pd.read_html(str(soup.find_all('table')))[3:-2]\n",
    "    table_3 = pd.concat(table_3).iloc[:, 0:2]\n",
    "    table_3 = table_3.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table = pd.concat([table_0, table_1, table_2, table_3])\n",
    "\n",
    "    table = table.loc[table['Tekst'].isin(['Anvendelse', 'Opførselsesår', 'Ombygningsår', 'Fredning',\n",
    "                                           'Køkkenforhold', 'Antal Etager', 'Antal toiletter', 'Antal badeværelser',\n",
    "                                           'Antal værelser',\n",
    "                                           'Ydervægsmateriale', 'Tagmateriale', 'Varmeinstallation',\n",
    "                                           'Bygning, Samlet areal', 'Boligstørrelse', 'Kælder', 'Vægtet Areal'])]\n",
    "    mydict = dict(zip(table.Tekst, list(table.Værdi)))\n",
    "    data.update(mydict)\n",
    "\n",
    "    try:\n",
    "        if 'ikke finde energimærke' in soup.find_all(\"div\", {\"id\": 'energimaerke'})[0].get_text():\n",
    "            data['Energimærke'] = ['Mangler']\n",
    "        else:\n",
    "            data['Energimærke'] = [soup.find_all(\"div\", {\"id\": 'energimaerke'})[0].find_all(\"p\")[0].get_text()[-3:-2]]\n",
    "        data['Indbrudsrisiko'] = [soup.find_all(\"div\", {\"id\": 'indbrud'})[0].find_all(\"u\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if 'ikke fredet' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = [0]\n",
    "        elif 'Bygningen er Bevaringsværdig' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = re.findall(r'\\d+', str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"p\")[4]))\n",
    "        elif 'Fejl ved opslag af' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = 'Mangler' #Seems to be flaw on site, all get mangler\n",
    "        else:\n",
    "            data['Bevaringsværdig'] = 'Ukendt'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['Største_parti'] = re.findall(r'valg/(.*?)(?<!\\\\).png',\n",
    "                                           str(soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all('h2')[0]))\n",
    "        data['Valgdeltagelse'] = \\\n",
    "        re.findall(\"\\d+.\\d+\", str(soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all('p')[1]))[1]\n",
    "        data['Afstemningsområde'] = [soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all(\"strong\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        url_vurdering = url + '/vurdering'\n",
    "        resp_vurdering = requests.get(url_vurdering)\n",
    "        soup_vurdering = BeautifulSoup(resp_vurdering.text, 'html.parser')\n",
    "        data['AVM_pris'] = \\\n",
    "        soup_vurdering.find_all(\"div\", {\"id\": 'avmnumber'})[0].get_text() #made correction\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # Make dataframe\n",
    "    df_page = pd.DataFrame(data)\n",
    "\n",
    "    return df_page\n",
    "\n",
    "#### Collect all scraped data from DinGeo for the addresses and ad to Boligsiden-dataframes\n",
    "\n",
    "def for_threading(url):\n",
    "\n",
    "    try:\n",
    "        df_pages = dingeo_page(url)\n",
    "        # df_geo = pd.concat([df_geo, df_pages])\n",
    "        #   time.sleep(1)\n",
    "        return df_pages\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def add_dingeo(df):\n",
    "\n",
    "    url_list = df['dingeo_link'].tolist()\n",
    "\n",
    "    df_geo = pd.DataFrame()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = tqdm(executor.map(for_threading, url_list))\n",
    "\n",
    "        for result in results:\n",
    "            df_geo = pd.concat([df_geo, result])\n",
    "\n",
    "\n",
    "    df_Boligsiden_Dingeo = pd.merge(df, df_geo, how='inner', on='dingeo_link', right_index=False).drop_duplicates()\n",
    "\n",
    "    return df_Boligsiden_Dingeo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Functions for scraping hvorlangterder.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape information for single address from hvorlangterder.dk\n",
    "\n",
    "def get_hvorlangterder(address):\n",
    "    try:\n",
    "        url = 'https://hvorlangterder.poi.viamap.net/v1/nearestpoi/?poitypes' \\\n",
    "              '=daycare,doctor,hospital,junction,metro,school,stop,strain,supermarket,train,library,pharmacy,coast' \\\n",
    "              ',forest,lake,airport,sportshall,publicbath,soccerfield,roadtrain&fromaddress=' + address \\\n",
    "              + '&mot=foot&token=eyJkcGZ4IjogImh2b3JsYW5ndGVyZGVyIiwgInByaXZzIjogInIxWjByMEYwazZCdFdxUWNPVXlrQi95N' \\\n",
    "                'lNVcEp2MlFiZ3lYZXRxNEhZNFhPLzNZclcwK0s5dz09In0.fP4JWis69HmaSg5jVHiK8nemiCu6VaMULSGGJyK4D4PkWq4iA1' \\\n",
    "                '+nSHWMaHxepKwJ83sEiy9nMNZhv7BcktRNrA'\n",
    "        resp = requests.get(url)\n",
    "        cont = resp.json()\n",
    "        df = pd.DataFrame(cont).loc[['routedmeters']]\n",
    "        df['Location'] = address\n",
    "\n",
    "        return (df)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#### Scrape data from hvorlangterder.dk for all adresses and merge with data from Boligsiden and DinGeo.dk\n",
    "    \n",
    "def add_hvorlangterder(df):\n",
    "\n",
    "\n",
    "    df_hvorlangt = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        try:\n",
    "            data = get_hvorlangterder(str(df['Location'][i]))\n",
    "            df_hvorlangt = pd.concat([df_hvorlangt, data])\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.2)\n",
    "\n",
    "\n",
    "    merged = pd.merge(df, df_hvorlangt, how='inner', on='Location', right_index=False).drop_duplicates()\n",
    "    return merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Collecting all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # All code below is commented out as not to risk running the very time consuming scraping process again\n",
    "    \n",
    "    \n",
    "    ############### Get Boligsiden-data ###############\n",
    "\n",
    "    # links = get_all_links_boligsiden('København', '2016-01-01', '2020-12-31') # This was also done for Frederiksberg\n",
    "    #\n",
    "    # with open('links_boligsiden_K.txt', 'w') as file:\n",
    "    #        file.write(str(links))\n",
    "    #\n",
    "    # with open(\"links_boligsiden_K.txt\", \"r\") as file:\n",
    "    #   links = eval(file.readline())\n",
    "    #\n",
    "    # df1, df2 = get_data_boligsiden(links[30000:])\n",
    "    # df1.to_csv('boligsiden_1_K7.csv', index=False)\n",
    "    # df2.to_csv('boligsiden_2_K7.csv', index=False)\n",
    "    # # The above was done 7 times for Copenhagen and 1 time for Frederiksberg, as to keep data frames small\n",
    "\n",
    "\n",
    "    # # This gives data for 41024, where the df2's are dropped since they have missing data.\n",
    "    # # In most cases this is due to that the unit is still for sale.\n",
    "    # # Other addresses were likewise dropped due to problem with Boligsiden page.\n",
    "\n",
    "\n",
    "    ############### Get Dingeo-data ##################\n",
    "\n",
    "    # df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K7.csv\")\n",
    "    # df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1) #[:]) #Here we choose how many to do at a time\n",
    "    # df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "    # # pd.set_option('display.max_columns', None)\n",
    "    # # print(df_Boligsiden_Dingeo1)\n",
    "    # df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K7.csv', index=False)\n",
    "\n",
    "    # # Data is lost for adresses where there is a problem with the address name or missing data on the DinGeo-page.\n",
    "    # # This results in a data set of 40657 addresses\n",
    "\n",
    "    ########## Get Hvorlangterder data ##############\n",
    "\n",
    "    # geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K7.csv\")\n",
    "    # geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "    #                        + geo_bolig1['address.postalId'].astype(str)\n",
    "    # df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "    # df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K7.csv', index=False)\n",
    "\n",
    "    # # Again loss of addresses, where no information is available on the hvorlangerder.dk web page\n",
    "    # # This finally gives 40606 adresses.\n",
    "\n",
    "    ########### Creating final raw data frame ##########\n",
    "\n",
    "    # bdh_1_F = pd.read_csv(\"bdh_1_F.csv\")\n",
    "    # bdh_1_K1 = pd.read_csv(\"bdh_1_K1.csv\")\n",
    "    # bdh_1_K2 = pd.read_csv(\"bdh_1_K2.csv\")\n",
    "    # bdh_1_K3 = pd.read_csv(\"bdh_1_K3.csv\")\n",
    "    # bdh_1_K4 = pd.read_csv(\"bdh_1_K4.csv\")\n",
    "    # bdh_1_K5 = pd.read_csv(\"bdh_1_K5.csv\")\n",
    "    # bdh_1_K6 = pd.read_csv(\"bdh_1_K6.csv\")\n",
    "    # bdh_1_K7 = pd.read_csv(\"bdh_1_K7.csv\")\n",
    "\n",
    "    #\n",
    "    # raw_data_1 = pd.concat([bdh_1_F, bdh_1_K1, bdh_1_K2, bdh_1_K3, bdh_1_K4,\n",
    "    #                         bdh_1_K5, bdh_1_K6, bdh_1_K7], sort=False)\n",
    "    # raw_data_1.to_csv('raw_data_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
